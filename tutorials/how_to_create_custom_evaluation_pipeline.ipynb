{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ourownstory/neural_prophet/blob/master/tutorials/UnderstandeTheBenchmarkingPipeline.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a custom evaluation pipeline\n",
    "This tutorial takes you behind the scenes of the benchmark template and guides you in creating your custom evaluation\n",
    "pipeline by explaining the processing step-by-step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install git+https://github.com/ourownstory/test-of-time.git # may take a while\n",
    "    #!pip install test-of-time # much faster, but may not have the latest upgrades/bugfixes\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from neuralprophet import set_log_level, set_random_seed\n",
    "\n",
    "from tot.df_utils import _check_min_df_len, prep_or_copy_df, check_dataframe, handle_missing_data, split_df, return_df_in_original_format, maybe_drop_added_dates\n",
    "from tot.models.models_neuralprophet import NeuralProphetModel\n",
    "from tot.data_processing.scaler import Scaler\n",
    "from tot.evaluation.metric_utils import calculate_averaged_metrics_per_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_log_level(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark templates test-of-time offers are a quick and simple way to compare multiple models and datasets.\n",
    "Defining a benchmark and running it will trigger a pipeline that returns the benchmark results. The benchmark\n",
    "is sub-divided into multiple experiments, that are executed in the pipeline. Every experiment run follows the\n",
    "same evaluation steps. Let's have a closer look at the individual steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation processing steps\n",
    "1. Data-specific pre-processing:\n",
    "    Remark: This processing is independent of the model and specific to the data)\n",
    "    - Prepare dataframe to have an ID column\n",
    "    - Performs a basic sanity check\n",
    "    - Handles missing data\n",
    "    - Splits the data into train and test datasets\n",
    "    - [optional] Data transformation (standarization, scaling, etc)\n",
    "\n",
    "2. Model definition\n",
    "    - Set random seed to ensure reproducibility\n",
    "    - define the model parameters\n",
    "    - instantiate your model\n",
    "\n",
    "3. Model-specific data pre-processing:\n",
    "    - Adjusts the data according to the model configuration to have each model be fitted and predicted correctly\n",
    "\n",
    "4. Fit model:\n",
    "    - Calls the fit method on the instantiated model object\n",
    "\n",
    "5. Predict model:\n",
    "    - Calls the predict method to create the forecast\n",
    "\n",
    "6. Model-specific data post-processing:\n",
    "    - Adjusts the data according to the model configuration to be returned consistently\n",
    "\n",
    "7. Data-specific post-processing:\n",
    "    - Drops any added dates\n",
    "    - [optional] Data inverse transformation\n",
    "\n",
    "8. Evaluation:\n",
    "    - Evaluates the forecasts based on selected error metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Let's load the AirPassenger dataset as an example dataset to walk through the pipeline step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = \"https://raw.githubusercontent.com/ourownstory/neuralprophet-data/main/datasets/\"\n",
    "df_air = pd.read_csv(data_location + 'air_passengers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data-specific pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_or_copy_df() ensures that the df has an \"ID\" column to be usable in the further process\n",
    "df_air, received_ID_col, received_single_time_series, _ = prep_or_copy_df(df_air)\n",
    "# check_dataframe() performs a basic sanity check on the data\n",
    "df_air = check_dataframe(df_air, check_y=True)\n",
    "# handle_missing_data() imputes missing data\n",
    "df_air = handle_missing_data(df_air, freq='MS')\n",
    "# split_df() splits the data into train and test data\n",
    "df_air_train, df_air_test = split_df(\n",
    "    df=df_air,\n",
    "    test_percentage=0.40,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [optional] Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler(transformer=StandardScaler(), scaling_level=\"per_dataset\")\n",
    "df_air_train, df_air_test = scaler.transform(df_air_train, df_air_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(42)\n",
    "model_class = NeuralProphetModel\n",
    "params =  {\n",
    "    \"n_forecasts\": 3,\n",
    "    \"n_lags\":7,\n",
    "    \"seasonality_mode\": \"multiplicative\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"_data_params\":{},\n",
    "    \"normalize\": \"off\", # normalization should be disabled when normalizing data in the pre-processing step\n",
    "}\n",
    "model=model_class(params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model-specific data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if train and test df contain enough samples\n",
    "_check_min_df_len(df=df_air_train, min_len=model.n_forecasts + model.n_lags)\n",
    "_check_min_df_len(df=df_air_test, min_len=model.n_forecasts)\n",
    "# extend the test df with historic values from the train df\n",
    "df_air_test_extended = model.maybe_extend_df(df_air_train, df_air_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.fit(df=df_air_train, freq='MS', progress=\"none\", minimal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predict model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model-individual predict function outputs the forecasts as a df\n",
    "fcst_train = model.model.predict(df=df_air_train)\n",
    "fcst_test = model.model.predict(df=df_air_test_extended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model-specific post-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, the method is a class method and hence linked to the model\n",
    "fcst_test = model.maybe_drop_added_values_from_df(fcst_test, df_air_test_extended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Data-specific data post-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case, missing data was imputed maybe_drop_added_dates() removes it again\n",
    "fcst_train_df, df_air_train = maybe_drop_added_dates(fcst_train, df_air_train)\n",
    "fcst_test_df, df_air_test = maybe_drop_added_dates(fcst_test, df_air_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [optional] Data inverse transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_train, fcst_test = scaler.inverse_transform(fcst_train, fcst_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAPE': 5.789534623424212}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_train = calculate_averaged_metrics_per_experiment(\n",
    "    fcst_df=fcst_train, df_historic=fcst_train, metrics=[\"MAPE\"], metadata={}, freq=\"MS\")\n",
    "result_test = calculate_averaged_metrics_per_experiment(\n",
    "    fcst_df=fcst_test, df_historic=fcst_train, metrics=[\"MAPE\"], metadata={}, freq=\"MS\")\n",
    "result_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tot",
   "language": "python",
   "name": "tot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
